# 典型回答
大模型的“幻觉”指的是 AI 生成了看似合理但实际上错误或编造的信息。例如，它可能会编造不存在的事实、错误引用文献、甚至捏造公司或人物的信息。



### 幻觉产生原因


#### 语言模型的“填空”机制
+ Transformer 语言模型本质上是一个 **“填空预测器”**，它是根据概率预测来选择下一个输出的词，而不是在“思考”正确答案。



#### 训练数据存在缺陷
+ 训练数据本身可能包含 **错误信息、不完整数据、偏见信息**，导致模型学到不真实的内容。
+ 训练数据可能过时，例如，GPT-4 的数据 **截止到 2023 年初**，无法回答最新时事。



#### 缺乏事实验证能力
+ 语言模型在生成文本时，并不会主动去 **查证答案的真实性**。



#### 长文本记忆力有限
+ 由于 **上下文窗口有限**（如 GPT-4-turbo 约 **128k tokens**），当文本过长时，AI **可能遗忘前面提到的信息**。





### 解决幻觉的几个方案


1、RAG，通过RAG的方式，让大模型在回答问题之前先检索真实数据，再让模型进行回答



2、Fine-tuning，即微调，通过微调的方式，给模型学习专业领域的知识，让他更好的回答



3、限制AI的回答，比如在提示词中告诉他如果你不知道，直接就回答不知道



4、通过标注和反馈不断优化模型。并把反馈可以给到模型让模型调整。



5、让同一个模型多次生成同一个内容的答案，然后选择一个最终版本。



6、在问题回答之后，让 AI 自己检查自己的答案，并标记不确定的部分。  



7、联网， 在 AI 生成答案之前，先通过网络查询最新数据。



8、 让 AI先写下推理过程，再得出最终结论，而不是直接给出答案。  

