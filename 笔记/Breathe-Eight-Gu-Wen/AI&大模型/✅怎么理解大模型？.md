所谓的大模型通常指基于深度学习的大规模人工智能模型，尤其是像 GPT-4这样的大语言模型（LLM, Large Language Model）

****

**相比于传统模型，大模型的"大"主要体现在就是参数规模大**。传统深度学习模型的参数规模通常在百万级到千万级，而大模型的参数量达到了 **百亿级、千亿级，甚至万亿级**。以 GPT-4 为例，参数规模可能达 **万亿级**。



> 如我们通常说的7B模型，指的就是他的参数量达到了70亿。
>



另外大模型的核心是基于 **深度学习** 和 **神经网络**，通常采用 **Transformer** 架构。



> Transformer 通过 “自注意力机制（Self-Attention）” 和 “多头注意力机制（Multi-Head Attention）” 实现高效的信息处理，相比于传统的 RNN（循环神经网络）和 CNN（卷积神经网络）具有更强的并行计算能力和更长的上下文理解能力。  （了解即可，不用背）
>



但是不要简单的认为大模型就只是AI对话，他的应用方向非常广泛，涵盖 **自然语言处理（NLP）、计算机视觉（CV）、语音识别、科学计算、自动驾驶等** 领域。  



| **应用领域** | **代表应用** | **典型模型** |
| --- | --- | --- |
| **1. NLP 语言 AI** | ChatGPT、翻译、代码生成 | GPT-4、LLaMA 3、Claude |
| **2. 计算机视觉（CV）** | 图像生成、目标检测 | Stable Diffusion、DALL·E、ViT |
| **3. 语音 AI** | 语音识别、语音克隆 | Whisper、VALL-E、Tacotron |
| **4. 自动驾驶 & 机器人** | 无人车、机器人导航 | Tesla FSD、Gato、Perceiver |
| **5. 科学计算 & 医疗** | 药物研发、医学影像 | AlphaFold、Med-PaLM 2 |
| **6. 推荐系统 & 广告** | 电商推荐、精准营销 | DeepFM、DINO、YouTube AI |




目前，大模型遇到的一些关键问题有以下几个：



+ **数据和隐私问题**：大模型训练涉及大量数据，可能存在隐私泄露风险。
+ **幻觉：模型可能生成虚假或不准确的信息。**
+ **能耗高：训练一次大模型可能消耗数千吨二氧化碳当量的能源。**
+ **可控性和安全性：如何让模型可靠、透明地运作是一个重要挑战。**



